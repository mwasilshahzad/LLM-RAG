{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2d254849-5d4c-465f-9b1a-2eea62d7245d)')' thrown while requesting HEAD https://huggingface.co/gpt2/resolve/main/vocab.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<qdrant_client.qdrant_client.QdrantClient object at 0x0000026F0F4CF610>\n",
      "--------------------------------\n",
      "<langchain.vectorstores.qdrant.Qdrant object at 0x0000026F104B5760>\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.8593211, 'content': 'Similarity score: 0.8593. Document: 2.Overview of individual tree crown detection methods \\nClassical approaches to individual tree crown detection (see (Skur-\\nikhin et al., 2013 ; Hung et al., 2012 )) use pattern recognition algorithms \\nto extract handcrafted tree crown-like features, such as local maximum \\nfiltering (Xu et al., 2021b ; Gebreslasie et al., 2011 ; Zheng et al., 2022b ),', 'metadata': {'page': 3, 'source': 'data.pdf'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.84992313, 'content': 'Similarity score: 0.8499. Document: regions. Panagiotidis et al. (2017) combine local maximum filtering and \\ninverse watershed segmentation to estimate crown diameters, achieving \\nan acceptable accuracy for detecting tree crown diameter. Software \\ntools are available for some classical approaches (Gebreslasie et al., \\n2011 ; Santoso et al., 2016 ) but their utility is limited by the need to tune \\nmany parameters and their lack', 'metadata': {'page': 3, 'source': 'data.pdf'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.8391424, 'content': 'Similarity score: 0.8391. Document: pling, respectively. Grid R-CNN (Lu et al., 2019 ) mainly adopts a grid \\nguided localization scheme to attain outstanding object detection re-\\nsults. MOPAD (Zheng et al., 2021a ) combines a Refined Pyramid Feature \\n(RPF) module and a hybrid class-balanced loss module to achieve \\nsatisfying observation of the growing status of individual tree crowns. ', 'metadata': {'page': 7, 'source': 'data.pdf'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.83477336, 'content': 'Similarity score: 0.8348. Document: because each tree is coarsely pixelated in the imagery, and densely \\npacked canopies make it difficult to distinguish individuals. This makes \\ntree detection distinct from other object detection tasks, such as \\ndetecting cats or dogs from photographs in the COCO dataset, where \\neach object comprises thousands of pixels. Fig. 3 displays comparison of \\nthe complexity and difficulty in object size and density between a ', 'metadata': {'page': 1, 'source': 'data.pdf'}}\n",
      "{'score': 0.8341498, 'content': 'Similarity score: 0.8341. Document: 2020 ), or exclusion of certain objects (Brandt et al., 2020 ) is frequently \\nrequired. Generally, the object detection-based algorithms are more \\nefficient and more accurate than existing individual tree crown detec -\\ntion algorithms, eliminating performance reduction caused by difficult \\nterrain and confusion among plant types. \\nAs shown in Table 1, existing coconut tree detection mainly adopted \\nsliding-window technique to achieve coconut', 'metadata': {'page': 3, 'source': 'data.pdf'}}\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 model and tokenizer for text generation\n",
    "generation_model_name = \"gpt2\"\n",
    "generation_model = GPT2LMHeadModel.from_pretrained(generation_model_name)\n",
    "generation_tokenizer = GPT2Tokenizer.from_pretrained(generation_model_name)\n",
    "\n",
    "# Specify device for generation model\n",
    "generation_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "generation_model.to(generation_device)\n",
    "\n",
    "# Set up Qdrant and embeddings\n",
    "model_name = \"BAAI/bge-large-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "url = \"http://localhost:6333\"\n",
    "collection_name = \"gpt_db\"\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=url,\n",
    "    prefer_grpc=False\n",
    ")\n",
    "\n",
    "print(client)\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "db = Qdrant(\n",
    "    client=client,\n",
    "    embeddings=embeddings,\n",
    "    collection_name=collection_name\n",
    ")\n",
    "\n",
    "print(db)\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# Query for similar documents\n",
    "query = \"What are classical approaches to tree detection problem?\"\n",
    "docs = db.similarity_search_with_score(query=query, k=5)\n",
    "\n",
    "# Generate text using GPT-2 model for each retrieved document\n",
    "for i in docs:\n",
    "    doc, score = i\n",
    "    prompt = f\"Similarity score: {score:.4f}. Document: {doc.page_content}.\"\n",
    "\n",
    "    # Tokenize and generate text\n",
    "    input_ids = generation_tokenizer.encode(prompt, return_tensors=\"pt\").to(generation_device)\n",
    "    output_ids = generation_model.generate(input_ids, max_length=200, num_beams=5, no_repeat_ngram_size=2)\n",
    "    \n",
    "    # Manually truncate the output sequence to the specified max_length\n",
    "    output_ids = output_ids[:, :100]  # Adjust the length as needed\n",
    "\n",
    "    generated_text = generation_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print({\"score\": score, \"content\": generated_text, \"metadata\": doc.metadata})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-rag-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration\n",
    "import torch\n",
    "\n",
    "# Set up BGE (Big Graph Embeddings) model\n",
    "bge_model_name = \"BAAI/bge-large-en\"\n",
    "bge_model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "\n",
    "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=bge_model_name,\n",
    "    model_kwargs=bge_model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Connect to Qdrant\n",
    "qdrant_url = \"http://localhost:6333\"\n",
    "collection_name = \"gpt_db\"\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    url=qdrant_url,\n",
    "    prefer_grpc=False\n",
    ")\n",
    "\n",
    "# Initialize Qdrant with BGE embeddings\n",
    "bge_db = Qdrant(\n",
    "    client=qdrant_client,\n",
    "    embeddings=bge_embeddings,\n",
    "    collection_name=collection_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGE DB: <langchain.vectorstores.qdrant.Qdrant object at 0x0000012A7C78F790>\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"BGE DB:\", bge_db)\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.8593211, 'content': '2.Overview of individual tree crown detection methods \\nClassical approaches to individual tree crown detection (see (Skur-\\nikhin et al., 2013 ; Hung et al., 2012 )) use pattern recognition algorithms \\nto extract handcrafted tree crown-like features, such as local maximum \\nfiltering (Xu et al., 2021b ; Gebreslasie et al., 2011 ; Zheng et al., 2022b ), \\nimage binarization (Koc-San et al., 2018 ; Pitk¨anen, 2001 ), image seg-\\nmentation (Gougeon and Leckie, 2006 ; Santoso et al., 2016 ; Miraki', 'metadata': {'page': 3, 'source': 'data.pdf'}}\n",
      "{'score': 0.84992313, 'content': 'regions. Panagiotidis et al. (2017) combine local maximum filtering and \\ninverse watershed segmentation to estimate crown diameters, achieving \\nan acceptable accuracy for detecting tree crown diameter. Software \\ntools are available for some classical approaches (Gebreslasie et al., \\n2011 ; Santoso et al., 2016 ) but their utility is limited by the need to tune \\nmany parameters and their lack of generality (Pitk¨anen, 2001 ). Many', 'metadata': {'page': 3, 'source': 'data.pdf'}}\n",
      "{'score': 0.8391424, 'content': 'pling, respectively. Grid R-CNN (Lu et al., 2019 ) mainly adopts a grid \\nguided localization scheme to attain outstanding object detection re-\\nsults. MOPAD (Zheng et al., 2021a ) combines a Refined Pyramid Feature \\n(RPF) module and a hybrid class-balanced loss module to achieve \\nsatisfying observation of the growing status of individual tree crowns. \\nTable 3 \\nThe precision, recall and F1 score for four atolls using deep learning based tree \\ndetection methods.', 'metadata': {'page': 7, 'source': 'data.pdf'}}\n",
      "{'score': 0.83477336, 'content': 'because each tree is coarsely pixelated in the imagery, and densely \\npacked canopies make it difficult to distinguish individuals. This makes \\ntree detection distinct from other object detection tasks, such as \\ndetecting cats or dogs from photographs in the COCO dataset, where \\neach object comprises thousands of pixels. Fig. 3 displays comparison of \\nthe complexity and difficulty in object size and density between a \\ncommon object detection problem in the COCO dataset and a tree', 'metadata': {'page': 1, 'source': 'data.pdf'}}\n",
      "{'score': 0.8341498, 'content': '2020 ), or exclusion of certain objects (Brandt et al., 2020 ) is frequently \\nrequired. Generally, the object detection-based algorithms are more \\nefficient and more accurate than existing individual tree crown detec -\\ntion algorithms, eliminating performance reduction caused by difficult \\nterrain and confusion among plant types. \\nAs shown in Table 1, existing coconut tree detection mainly adopted \\nsliding-window technique to achieve coconut tree counting in a picture,', 'metadata': {'page': 3, 'source': 'data.pdf'}}\n"
     ]
    }
   ],
   "source": [
    "# Sample query\n",
    "query = \"What are classical approaches to tree detection problem?\"\n",
    "\n",
    "# Perform similarity search with BGE embeddings\n",
    "bge_docs = bge_db.similarity_search_with_score(query=query, k=5)\n",
    "\n",
    "for i in bge_docs:\n",
    "    doc, score = i\n",
    "    print({\"score\": score, \"content\": doc.page_content, \"metadata\": doc.metadata})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant embeddings from the result\n",
    "# Handle the case where bge_docs is a list of tuples\n",
    "relevant_embeddings = [doc[0].embedding if isinstance(doc, tuple) else doc.embedding for doc in bge_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 4.60k/4.60k [00:00<?, ?B/s]\n",
      "c:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\site-packages\\transformers\\models\\bart\\configuration_bart.py:179: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<?, ?B/s]\n",
      "Downloading (…)_tokenizer/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 638kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 7.17kB/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<?, ?B/s]\n",
      "Downloading (…)tokenizer/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 1.36MB/s]\n",
      "Downloading (…)tokenizer/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.87MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 772/772 [00:00<00:00, 139kB/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "Downloading builder script: 100%|██████████| 9.62k/9.62k [00:00<?, ?B/s]\n",
      "Downloading metadata: 100%|██████████| 67.5k/67.5k [00:00<00:00, 479kB/s]\n",
      "Downloading readme: 100%|██████████| 14.6k/14.6k [00:00<?, ?B/s]\n",
      "Downloading data:  94%|█████████▍| 4.43G/4.69G [36:50<02:09, 2.01MB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\Projects\\Projects\\MLAI\\LLM-RAG\\vector_db.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/Projects/Projects/MLAI/LLM-RAG/vector_db.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Set up RAG model components\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/Projects/Projects/MLAI/LLM-RAG/vector_db.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m rag_tokenizer \u001b[39m=\u001b[39m RagTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mfacebook/rag-token-nq\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects/Projects/Projects/MLAI/LLM-RAG/vector_db.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m rag_retriever \u001b[39m=\u001b[39m RagRetriever\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mfacebook/rag-token-nq\u001b[39;49m\u001b[39m\"\u001b[39;49m, index_name\u001b[39m=\u001b[39;49mcollection_name)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/Projects/Projects/MLAI/LLM-RAG/vector_db.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m rag_sequence_generator \u001b[39m=\u001b[39m RagSequenceForGeneration\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mfacebook/rag-token-nq\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\site-packages\\transformers\\models\\rag\\retrieval_rag.py:429\u001b[0m, in \u001b[0;36mRagRetriever.from_pretrained\u001b[1;34m(cls, retriever_name_or_path, indexed_dataset, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m     index \u001b[39m=\u001b[39m CustomHFIndex(config\u001b[39m.\u001b[39mretrieval_vector_size, indexed_dataset)\n\u001b[0;32m    428\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 429\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_build_index(config)\n\u001b[0;32m    430\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\n\u001b[0;32m    431\u001b[0m     config,\n\u001b[0;32m    432\u001b[0m     question_encoder_tokenizer\u001b[39m=\u001b[39mquestion_encoder_tokenizer,\n\u001b[0;32m    433\u001b[0m     generator_tokenizer\u001b[39m=\u001b[39mgenerator_tokenizer,\n\u001b[0;32m    434\u001b[0m     index\u001b[39m=\u001b[39mindex,\n\u001b[0;32m    435\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\site-packages\\transformers\\models\\rag\\retrieval_rag.py:409\u001b[0m, in \u001b[0;36mRagRetriever._build_index\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[39mreturn\u001b[39;00m CustomHFIndex\u001b[39m.\u001b[39mload_from_disk(\n\u001b[0;32m    404\u001b[0m         vector_size\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mretrieval_vector_size,\n\u001b[0;32m    405\u001b[0m         dataset_path\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mpassages_path,\n\u001b[0;32m    406\u001b[0m         index_path\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mindex_path,\n\u001b[0;32m    407\u001b[0m     )\n\u001b[0;32m    408\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[39mreturn\u001b[39;00m CanonicalHFIndex(\n\u001b[0;32m    410\u001b[0m         vector_size\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mretrieval_vector_size,\n\u001b[0;32m    411\u001b[0m         dataset_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mdataset,\n\u001b[0;32m    412\u001b[0m         dataset_split\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mdataset_split,\n\u001b[0;32m    413\u001b[0m         index_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mindex_name,\n\u001b[0;32m    414\u001b[0m         index_path\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mindex_path,\n\u001b[0;32m    415\u001b[0m         use_dummy_dataset\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49muse_dummy_dataset,\n\u001b[0;32m    416\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\site-packages\\transformers\\models\\rag\\retrieval_rag.py:264\u001b[0m, in \u001b[0;36mCanonicalHFIndex.__init__\u001b[1;34m(self, vector_size, dataset_name, dataset_split, index_name, index_path, use_dummy_dataset)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_dummy_dataset \u001b[39m=\u001b[39m use_dummy_dataset\n\u001b[0;32m    263\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading passages from \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 264\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\n\u001b[0;32m    265\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_name, with_index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, split\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_split, dummy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_dummy_dataset\n\u001b[0;32m    266\u001b[0m )\n\u001b[0;32m    267\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(vector_size, dataset, index_initialized\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\site-packages\\datasets\\load.py:2153\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   2150\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[0;32m   2152\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 2153\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[0;32m   2154\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[0;32m   2155\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[0;32m   2156\u001b[0m     verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[0;32m   2157\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[0;32m   2158\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[0;32m   2159\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   2160\u001b[0m )\n\u001b[0;32m   2162\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   2163\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[0;32m   2164\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[0;32m   2165\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\site-packages\\datasets\\builder.py:954\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    952\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    953\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[1;32m--> 954\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_and_prepare(\n\u001b[0;32m    955\u001b[0m         dl_manager\u001b[39m=\u001b[39mdl_manager,\n\u001b[0;32m    956\u001b[0m         verification_mode\u001b[39m=\u001b[39mverification_mode,\n\u001b[0;32m    957\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_split_kwargs,\n\u001b[0;32m    958\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[0;32m    959\u001b[0m     )\n\u001b[0;32m    960\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[0;32m    961\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\site-packages\\datasets\\builder.py:1717\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[0;32m   1716\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_download_and_prepare\u001b[39m(\u001b[39mself\u001b[39m, dl_manager, verification_mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_splits_kwargs):\n\u001b[1;32m-> 1717\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_download_and_prepare(\n\u001b[0;32m   1718\u001b[0m         dl_manager,\n\u001b[0;32m   1719\u001b[0m         verification_mode,\n\u001b[0;32m   1720\u001b[0m         check_duplicate_keys\u001b[39m=\u001b[39mverification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS\n\u001b[0;32m   1721\u001b[0m         \u001b[39mor\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS,\n\u001b[0;32m   1722\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_splits_kwargs,\n\u001b[0;32m   1723\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\site-packages\\datasets\\builder.py:1027\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m   1025\u001b[0m split_dict \u001b[39m=\u001b[39m SplitDict(dataset_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_name)\n\u001b[0;32m   1026\u001b[0m split_generators_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[1;32m-> 1027\u001b[0m split_generators \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_generators(dl_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msplit_generators_kwargs)\n\u001b[0;32m   1029\u001b[0m \u001b[39m# Checksums verification\u001b[39;00m\n\u001b[0;32m   1030\u001b[0m \u001b[39mif\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS \u001b[39mand\u001b[39;00m dl_manager\u001b[39m.\u001b[39mrecord_checksums:\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\wiki_dpr\\74d4bff38a7c18a9498fafef864a8ba7129e27cb8d71b22f5e14d84cb17edd54\\wiki_dpr.py:130\u001b[0m, in \u001b[0;36mWikiDpr._split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_split_generators\u001b[39m(\u001b[39mself\u001b[39m, dl_manager):\n\u001b[0;32m    129\u001b[0m     files_to_download \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mdata_file\u001b[39m\u001b[39m\"\u001b[39m: _DATA_URL}\n\u001b[1;32m--> 130\u001b[0m     downloaded_files \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39;49mdownload_and_extract(files_to_download)\n\u001b[0;32m    131\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mwith_embeddings:\n\u001b[0;32m    132\u001b[0m         vectors_url \u001b[39m=\u001b[39m _NQ_VECTORS_URL \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39membeddings_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnq\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m _MULTISET_VECTORS_URL\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\site-packages\\datasets\\download\\download_manager.py:565\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[1;34m(self, url_or_urls)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload_and_extract\u001b[39m(\u001b[39mself\u001b[39m, url_or_urls):\n\u001b[0;32m    550\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Download and extract given `url_or_urls`.\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \n\u001b[0;32m    552\u001b[0m \u001b[39m    Is roughly equivalent to:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[39m        extracted_path(s): `str`, extracted paths of given URL(s).\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 565\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload(url_or_urls))\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\site-packages\\datasets\\download\\download_manager.py:428\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[1;34m(self, url_or_urls)\u001b[0m\n\u001b[0;32m    425\u001b[0m download_func \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download, download_config\u001b[39m=\u001b[39mdownload_config)\n\u001b[0;32m    427\u001b[0m start_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m--> 428\u001b[0m downloaded_path_or_paths \u001b[39m=\u001b[39m map_nested(\n\u001b[0;32m    429\u001b[0m     download_func,\n\u001b[0;32m    430\u001b[0m     url_or_urls,\n\u001b[0;32m    431\u001b[0m     map_tuple\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    432\u001b[0m     num_proc\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mnum_proc,\n\u001b[0;32m    433\u001b[0m     disable_tqdm\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m is_progress_bar_enabled(),\n\u001b[0;32m    434\u001b[0m     desc\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDownloading data files\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    435\u001b[0m )\n\u001b[0;32m    436\u001b[0m duration \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39m start_time\n\u001b[0;32m    437\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading took \u001b[39m\u001b[39m{\u001b[39;00mduration\u001b[39m.\u001b[39mtotal_seconds()\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39m\u001b[39m60\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m min\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\site-packages\\datasets\\utils\\py_utils.py:464\u001b[0m, in \u001b[0;36mmap_nested\u001b[1;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, types, disable_tqdm, desc)\u001b[0m\n\u001b[0;32m    462\u001b[0m     num_proc \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    463\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m num_proc \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(iterable) \u001b[39m<\u001b[39m parallel_min_length:\n\u001b[1;32m--> 464\u001b[0m     mapped \u001b[39m=\u001b[39m [\n\u001b[0;32m    465\u001b[0m         _single_map_nested((function, obj, types, \u001b[39mNone\u001b[39;00m, \u001b[39mTrue\u001b[39;00m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    466\u001b[0m         \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(iterable, disable\u001b[39m=\u001b[39mdisable_tqdm, desc\u001b[39m=\u001b[39mdesc)\n\u001b[0;32m    467\u001b[0m     ]\n\u001b[0;32m    468\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    469\u001b[0m     \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\site-packages\\datasets\\utils\\py_utils.py:465\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    462\u001b[0m     num_proc \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    463\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m num_proc \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(iterable) \u001b[39m<\u001b[39m parallel_min_length:\n\u001b[0;32m    464\u001b[0m     mapped \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 465\u001b[0m         _single_map_nested((function, obj, types, \u001b[39mNone\u001b[39;49;00m, \u001b[39mTrue\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m))\n\u001b[0;32m    466\u001b[0m         \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(iterable, disable\u001b[39m=\u001b[39mdisable_tqdm, desc\u001b[39m=\u001b[39mdesc)\n\u001b[0;32m    467\u001b[0m     ]\n\u001b[0;32m    468\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    469\u001b[0m     \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\site-packages\\datasets\\utils\\py_utils.py:367\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[39m# Singleton first to spare some computation\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, types):\n\u001b[1;32m--> 367\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data_struct)\n\u001b[0;32m    369\u001b[0m \u001b[39m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[39mif\u001b[39;00m rank \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m logging\u001b[39m.\u001b[39mget_verbosity() \u001b[39m<\u001b[39m logging\u001b[39m.\u001b[39mWARNING:\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\site-packages\\datasets\\download\\download_manager.py:454\u001b[0m, in \u001b[0;36mDownloadManager._download\u001b[1;34m(self, url_or_filename, download_config)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[39mif\u001b[39;00m is_relative_path(url_or_filename):\n\u001b[0;32m    452\u001b[0m     \u001b[39m# append the relative path to the base_path\u001b[39;00m\n\u001b[0;32m    453\u001b[0m     url_or_filename \u001b[39m=\u001b[39m url_or_path_join(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_path, url_or_filename)\n\u001b[1;32m--> 454\u001b[0m \u001b[39mreturn\u001b[39;00m cached_path(url_or_filename, download_config\u001b[39m=\u001b[39;49mdownload_config)\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\site-packages\\datasets\\utils\\file_utils.py:182\u001b[0m, in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     url_or_filename \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(url_or_filename)\n\u001b[0;32m    180\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[0;32m    181\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[1;32m--> 182\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[0;32m    183\u001b[0m         url_or_filename,\n\u001b[0;32m    184\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    185\u001b[0m         force_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mforce_download,\n\u001b[0;32m    186\u001b[0m         proxies\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mproxies,\n\u001b[0;32m    187\u001b[0m         resume_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mresume_download,\n\u001b[0;32m    188\u001b[0m         user_agent\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muser_agent,\n\u001b[0;32m    189\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mlocal_files_only,\n\u001b[0;32m    190\u001b[0m         use_etag\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muse_etag,\n\u001b[0;32m    191\u001b[0m         max_retries\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    192\u001b[0m         token\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mtoken,\n\u001b[0;32m    193\u001b[0m         ignore_url_params\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mignore_url_params,\n\u001b[0;32m    194\u001b[0m         storage_options\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[0;32m    195\u001b[0m         download_desc\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mdownload_desc,\n\u001b[0;32m    196\u001b[0m     )\n\u001b[0;32m    197\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[0;32m    198\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     output_path \u001b[39m=\u001b[39m url_or_filename\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\site-packages\\datasets\\utils\\file_utils.py:644\u001b[0m, in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, token, use_auth_token, ignore_url_params, storage_options, download_desc)\u001b[0m\n\u001b[0;32m    642\u001b[0m         fsspec_get(url, temp_file, storage_options\u001b[39m=\u001b[39mstorage_options, desc\u001b[39m=\u001b[39mdownload_desc)\n\u001b[0;32m    643\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 644\u001b[0m         http_get(\n\u001b[0;32m    645\u001b[0m             url,\n\u001b[0;32m    646\u001b[0m             temp_file,\n\u001b[0;32m    647\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    648\u001b[0m             resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[0;32m    649\u001b[0m             headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    650\u001b[0m             cookies\u001b[39m=\u001b[39;49mcookies,\n\u001b[0;32m    651\u001b[0m             max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[0;32m    652\u001b[0m             desc\u001b[39m=\u001b[39;49mdownload_desc,\n\u001b[0;32m    653\u001b[0m         )\n\u001b[0;32m    655\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mcache_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    656\u001b[0m shutil\u001b[39m.\u001b[39mmove(temp_file\u001b[39m.\u001b[39mname, cache_path)\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\site-packages\\datasets\\utils\\file_utils.py:419\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, cookies, timeout, max_retries, desc)\u001b[0m\n\u001b[0;32m    410\u001b[0m total \u001b[39m=\u001b[39m resume_size \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(content_length) \u001b[39mif\u001b[39;00m content_length \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[0;32m    412\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    413\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[0;32m    418\u001b[0m ) \u001b[39mas\u001b[39;00m progress:\n\u001b[1;32m--> 419\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m response\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m):\n\u001b[0;32m    420\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n\u001b[0;32m    421\u001b[0m         temp_file\u001b[39m.\u001b[39mwrite(chunk)\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    630\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\site-packages\\urllib3\\response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\http\\client.py:458\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    456\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    457\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 458\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    459\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    460\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\http\\client.py:502\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    497\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[0;32m    499\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[0;32m    504\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    505\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    506\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\Dev\\anaconda3\\envs\\llm-rag-1\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set up RAG model components\n",
    "rag_tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "rag_retriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", index_name=collection_name)\n",
    "rag_sequence_generator = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-nq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert relevant embeddings to torch tensors\n",
    "input_tensors = torch.tensor(relevant_embeddings)\n",
    "\n",
    "# Get the prompt token ids from the tokenizer\n",
    "input_ids = rag_tokenizer.encode(\"Question: \" + query, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text using RAG with BERT LLM\n",
    "rag_output = rag_sequence_generator(input_ids=input_ids, retriever_results=input_tensors)\n",
    "\n",
    "# Decode the generated output\n",
    "generated_text = rag_tokenizer.decode(rag_output[\"sequences\"][0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
